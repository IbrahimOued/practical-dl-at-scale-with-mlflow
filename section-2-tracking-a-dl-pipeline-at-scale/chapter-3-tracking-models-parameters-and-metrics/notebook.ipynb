{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Tracking Models, Parameters, and Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that MLflow can support multiple scenarios through the life cycle of DL models, it is common to use MLflow's capabilities incrementally. Usually, people start with MLflow tracking since it is easy to use and can handle many scenarios for reproducibility, provenance tracking, and auditing purposes.\n",
    "\n",
    "We will then take a deep dive into how we can track a model, along with its parameters and metrics, using MLflow's tracking and registry APIs. By the end of this chapter, you should feel comfortable using MLflow's tracking and registry APIs for various reproducibility and auditing purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up a full-fledged local MLflow tracking server**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of having a model registry is that we can register the model, version control the model, and prepare for model deployment into production. Therefore, this model registry will bridge the gap between offline experimentation and an online deployment production scenario. Thus, we need a full-fledged MLflow tracking server with the following stores to track the complete life cycle of a model:\n",
    "* **Backend store**: A **relational database backend is needed to support MLflow's storage of metadata** (metrics, parameters, and many others) about the experiment. This also allows the query capability of the experiment to be used. **We will use a MySQL database as a local backend store**.\n",
    "* **Artifact store**: An object store that can store arbitrary types of objects, such as serialized models, vocabulary files, figures, and many others. In a production environment, a popular choice is the AWS S3 store. We will use [**MinIO**](https://min.io/), a multi-cloud object store, as a local artifact store, which is fully compatible with the AWS S3 store API but can run on your laptop without you needing to access the cloud.\n",
    "\n",
    "To make this local setup as easy as possible, we will use the [docker-compose](https://docs.docker.com/compose/) tool with one line of command to start and stop a local full-fledged MLflow tracking server, as described in the following steps. The following steps will launch the local MLflow tracking server inside your local Docker container:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clone https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03.\n",
    "2. Change directory to the `mlflow_docker_setup` subfolder, which can be found under the chapter03 folder.\n",
    "3. Run the following command:\n",
    "```bash\n",
    "bash start_mlflow.sh\n",
    "```\n",
    "4. Go to `http://localhost/` to see the MLflow UI web page. Then, click the Models tab in the UI. Note that this tab would not work if you only had a local filesystem as the backend store for the MLflow tracking server. Hence, the MLflow UI's backend is now running on the Docker container service you just started, not a local filesystem\n",
    "5. Go to `http://localhost:9000/`, and the following screen should appear for the **MinIO** artifact store web UI. Enter `minio` for Access Key and `minio123` for Secret Key. These are defined in the `.env` file, under the `mlflow_docker_setup` folder\n",
    "\n",
    "At this point, you should have a full-fledged local MLflow tracking server running successfully! If you want to stop the server, simply type the following command:\n",
    "```bash\n",
    "bash stop_mlflow.sh\n",
    "```\n",
    "The Docker-based MLflow tracking server will stop. We are now ready to use this local MLflow server to track model provenance, parameters, and metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tracking model provenance**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provenance** tracking for digital artifacts has been long studied in the litterature. For example, when **you're using a piece of patient diagnosis data in the biomedical industry**, people usually want to know **where it comes from, what kind of processing and cleaning has been done to the data, who owns the data, and other history and lineage information** about the data. **The rise of ML/DL models for industrial and business scenarios in production makes provenance tracking a required functionality**. The different granularities of provenance tracking are critical for operationalizing and managing not just the data science offline experimentation, but also before/during/after the model is deployed in production. So, what needs to be tracked for provenance?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the open provenance tracking framework**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a general provenance tracking framework to understand the big picture of why provenance tracking is a major effort. The following diagram is based on the [Open Provenance Model Vocabulary Specification](http://open-biomed.sourceforge.net/opmv/ns.html):\n",
    "\n",
    "![Text](open_provenance_model_vocab_spec.jpg)\n",
    "\n",
    "In the preceding diagram, there are three important items:\n",
    "\n",
    "* **Artifacts**: Things that are produced or used by processes (**A1** and **A2**).\n",
    "* **Processes**: Actions that are performed by using or producing artifacts (**P1** and **P2**).\n",
    "* **Causal relationships**: Edges or relationships between artifacts and processes, such as used, ***wasGeneratedBy***, and ***wasDerivedFrom*** in the preceding diagram (**R1**, **R2**, and **R3**).\n",
    "\n",
    "Intuitively, this open provenance model (OPM) framework allows us to ask the following 5W1H (five Ws and one H) questions, as follows:\n",
    "\n",
    "![Alt text](types_prov_questions.png)\n",
    "\n",
    "Having a systematic provenance framework and a set of questions will help us learn how to track model provenance and provide answers to these questions. This will motivate us when we implement MLflow model tracking in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementing MLflow model tracking**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use an MLflow tracking server to answer most of these types of provenance questions if we implement both **MLflow logging** and **registry** for the DL model we use. First, let's review what MLflow provides in terms of model provenance tracking. MLflow provides two sets of APIs for model provenance:\n",
    "* **Logging API**: This **allows each run of the experiment or a model pipeline to log** the model artifcat into the artifact store\n",
    "* **Registry API**: This **allows a centralized location to track the version** of the model and the stages of the model's life cycle (**None, Archived, Staging**, or **Production**)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***DIFFERENCE BETWEEN MODEL LOGGING AND MODEL REGISTRY***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although every run of the experiment needs to be logged and the model needs to be saved in the artifact store, **not every instance of the model needs to be registered in the model registry**. That's because, for many early exploratory model experimentations, the model might not be good. Thus, it is not necessarily registered to track the version. **Only when a model has good offline performance and becomes a candidate for promoting to production do we need to register it in the model registry to go through the model promotion process**.\n",
    "\n",
    "Although MLflow's official API documentation **separates logging and registry into two components**, we will **refer to them together as model tracking functionality in MLflow**.\n",
    "\n",
    "Although auto-logging is powerful, there are two issues with the current version:\n",
    "\n",
    "We already saw MLflow's auto-logging for the DL model build previously, although auto-logging is powerful, there are two issues with the current version:\n",
    "\n",
    "* It does not automatically register the model to the model registry.\n",
    "* It does not work out of the box for the logged model to work directly with the original input data (in our case, an English sentence) if you just follow MLflow's suggestion to use the `mlflow.pyfunc.load_model` API to load the logged model. This is a limitation that's probably due to the experimental nature of the current auto-logging APIs in MLflow.\n",
    "\n",
    "Let's walk through an example to review MLflow's capabilities and auto-logging's limitations and how we can solve them:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set up the following environment variables in your Bash terminal, where your MinIO and MySQL-based Docker component is running:\n",
    "```bash\n",
    "export MLFLOW_S3_ENDPOINT_URL=http://localhost:9000\n",
    "export AWS_ACCESS_KEY=minio\n",
    "export AWS_SECRET_ACCESS_KEY=minio123\n",
    "```\n",
    "2. To follow along with this model tracking implementation, check out the dl_model_tracking.ipynb notebook file in VS Code by going to [this chapter's GitHub repository](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb).\n",
    "\n",
    "ote that, in the fourth cell of the dl_model_tracking.ipynb notebook, we need to point it to the correct and new MLflow tracking URI that we just set up in the Docker and define a new experiment, as follows:\n",
    "```py\n",
    "EXPERIMENT_NAME = \"mlflow_dl_model_chapter_03\"\n",
    "mlflow.set_tracking_uri('http://localhost')\n",
    "```\n",
    "\n",
    "3. We will still use the auto-logging capabilities provided by MLflow but we will assign the run with a variable name, `dl_model_tracking_run`:\n",
    "`mlflow.pytorch.autolog()`\n",
    "```py\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id, run_name=\"chapter03\") as dl_model_tracking_run:\n",
    "    params = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"val_size\": len(val_dataset),\n",
    "        \"metric_function\" : \"accuracy\",\n",
    "        \"device\": str(device),\n",
    "        \"num_gpus\": torch.cuda.device_count(),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"max_token_length\": max_token_length\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "\n",
    "    with open(\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(summary(model)))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"---⌛ Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate_epoch(model, val_dataloader, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, \"\n",
    "                f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save the trained model to MLflow.\n",
    "    mlflow.pytorch.log_model(model, artifact_path=\"model\", name=\"model\")\n",
    "\n",
    "    # --- 5. Testing ---\n",
    "    # Once the fine-tuning step is completed, we will test the accuracy\n",
    "    # of the model by running trainer.test():\n",
    "    print(\"\\nStarting testing...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1 = evaluate_epoch(model, test_dataloader, device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, \"\n",
    "            f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    # --- 6. Prediction Example ---\n",
    "    print(\"\\nExample Prediction on new data:\")\n",
    "    model.eval()\n",
    "    new_reviews = [\n",
    "        \"This movie was absolutely fantastic! I loved every moment of it.\",\n",
    "        \"Utterly disappointing. A complete waste of time.\",\n",
    "        \"It was okay, nothing special.\",\n",
    "        \"The acting was superb, but the plot was a bit weak.\"\n",
    "    ]\n",
    "\n",
    "    for review in new_reviews:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_token_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predicted_label_id = torch.argmax(logits, dim=-1).item()\n",
    "            predicted_sentiment = \"positive\" if predicted_label_id == 1 else \"negative\"\n",
    "            print(f\"Review: '{review}' -> Predicted Sentiment: {predicted_sentiment}\")\n",
    "```\n",
    "`dl_model_tracking_run` allows us to get the `run_id` parameter and other metadata about this run programmatically, as we will see in the next step. Once this code cell has been executed, we will have a trained model logged in the MLflow tracking server with all the required parameters and metrics. However, the model hasn't been registered yet. We can find the logged experiment in the MLflow web UI, along with all the relevant parameters and metrics, at http://localhost/#/experiments/1/runs/37a3fe9b6faf41d89001eca13ad6ca47. You can find the model artifacts in the MinIO storage backend. Go to http://localhost:9000/minio/mlflow/1/37a3fe9b6faf41d89001eca13ad6ca47/artifacts/model/ to see the storage UI.\n",
    "4. Retrieve the run_id parameter from dl_model_tracking_run, as well as other metadata, as follows:\n",
    "```py\n",
    "run_id = dl_model_tracking_run.info.run_id\n",
    "print(\"run_id: {}; lifecycle_stage: {}\".format(run_id,\n",
    "    mlflow.get_run(run_id).info.lifecycle_stage))\n",
    "```\n",
    "This will print out something like the following:\n",
    "```bash\n",
    "run_id: 37a3fe9b6faf41d89001eca13ad6ca47; lifecycle_stage: active\n",
    "```\n",
    "5. Retrieve the logged model by defining the logged model URI. This will allow us to reload the logged model at this specific location:\n",
    "```py\n",
    "logged_model = f'runs:/{run_id}/model'\n",
    "```\n",
    "6. se mlflow.pytorch.load_model and the following logged_model URI to load the model back into memory and make a new prediction for a given input sentence, as follows:\n",
    "```py\n",
    "model = mlflow.pytorch.load_model(logged_model)\n",
    "model.predict({'This is great news'})\n",
    "```\n",
    "This will output a model prediction label, as follows:\n",
    "```bash\n",
    "['positive']\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import python dotenv\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv('./mlflow_docker_setup/.env')\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://localhost:9000\"\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = AWS_SECRET_ACCESS_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Download Utility (similar to Flash's download_data) ---\n",
    "def download_and_extract_zip(url, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    zip_file_name = os.path.join(path, \"downloaded_data.zip\")\n",
    "\n",
    "    print(f\"Downloading data from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 # 1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(zip_file_name, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong during download\")\n",
    "        return\n",
    "\n",
    "    print(f\"Extracting {zip_file_name} to {path}...\")\n",
    "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path)\n",
    "    os.remove(zip_file_name) # Clean up the zip file\n",
    "    print(\"Download and extraction complete.\")\n",
    "\n",
    "# --- 2. Custom Dataset for IMDB (replaces TextClassificationData) ---\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Ensure 'sentiment' is mapped to numerical labels\n",
    "        # 0 for negative, 1 for positive\n",
    "        self.label_map = {'negative': 0, 'positive': 1}\n",
    "        self.dataframe['sentiment_label'] = self.dataframe['sentiment'].map(self.label_map)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.dataframe.loc[idx, 'review'])\n",
    "        label = self.dataframe.loc[idx, 'sentiment_label']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt', # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Training Function ---\n",
    "# \"\"\"\n",
    "# Once we have the data, we can now perform fine-tuning using a foundation model.\n",
    "# First, we declare classifier_model by calling TextClassifier with a backbone\n",
    "# assigned to prajjwal1/bert-tiny (which is a much smaller BERT-like pretrained\n",
    "# model located in the Hugging Face model repository: https://huggingface.co/prajjwal1/bert-tiny).\n",
    "# This means our model will be based on the bert-tiny model.\n",
    "# \"\"\"\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# --- 4. Evaluation Function ---\n",
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    # Get precision, recall, f1-score for positive class (label 1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true_labels, all_predictions, average='binary', pos_label=1\n",
    "    )\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://pl-flash-data.s3.amazonaws.com/imdb.zip...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a04ff539e34e9c9827b70d9e0b0a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/15.9M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/downloaded_data.zip to ./data/...\n",
      "Download and extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. & 2. Data Download and Preparation ---\n",
    "data_url = \"https://pl-flash-data.s3.amazonaws.com/imdb.zip\"\n",
    "data_path = \"./data/\"\n",
    "download_and_extract_zip(data_url, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(data_path, \"imdb/train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(data_path, \"imdb/valid.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_path, \"imdb/test.csv\"))\n",
    "\n",
    "# Determine number of classes from the sentiment column\n",
    "num_classes = train_df['sentiment'].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Initialize Tokenizer\n",
    "model_name = \"prajjwal1/bert-tiny\" # As used in Flash example\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_token_length = 128 # A common max length for BERT-tiny, adjust as needed\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = IMDBDataset(train_df, tokenizer, max_token_length)\n",
    "val_dataset = IMDBDataset(val_df, tokenizer, max_token_length)\n",
    "test_dataset = IMDBDataset(test_df, tokenizer, max_token_length)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Model Definition and Setup ---\n",
    "# Use AutoModelForSequenceClassification for classification tasks with Hugging Face models\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Learning Rate\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # Common learning rate for fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_id: 1\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"mlflow_dl_model_chapter_03\"\n",
    "mlflow.set_tracking_uri('http://localhost:80')\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "print(\"experiment_id:\", experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 16:59:07 WARNING mlflow.utils.autologging_utils: MLflow pytorch autologging is known to be compatible with 1.9.0 <= torch <= 2.6.0, but the installed version is 2.7.0+cu126. If you encounter errors during autologging, try upgrading / downgrading torch to a compatible version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: fd21aa738a3c4d108e54d6a68d5dc356\n",
      "\n",
      "Starting training...\n",
      "---⌛ Epoch 1/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4e96db1e02428ea4f57f1b20fc86ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5df23f3df34310b561a0857b2063ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6655\n",
      "Validation Loss: 0.6159, Accuracy: 0.6880, Precision: 0.6947, Recall: 0.6471, F1: 0.6701\n",
      "---⌛ Epoch 2/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9ace2f8aa94e4a8c57707c42affb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ab07e6daf24906bbc4ed320d52807b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5721\n",
      "Validation Loss: 0.5051, Accuracy: 0.7592, Precision: 0.7645, Recall: 0.7345, F1: 0.7492\n",
      "---⌛ Epoch 3/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e9a82b5da7467e83edb3864ca387cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ef048943c9465fa3b793f022c383a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4823\n",
      "Validation Loss: 0.4627, Accuracy: 0.7788, Precision: 0.7599, Recall: 0.8015, F1: 0.7801\n",
      "---⌛ Epoch 4/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f15c21eaeff4f78afc358729435c87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909dbf6ed7da4d6dbe86564a8c55ff68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4370\n",
      "Validation Loss: 0.4524, Accuracy: 0.7860, Precision: 0.7480, Recall: 0.8489, F1: 0.7953\n",
      "---⌛ Epoch 5/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4eaa9fe80ef43e39a788106538f9061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cefa76481ab4848b45e3721e5ac447e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4090\n",
      "Validation Loss: 0.4365, Accuracy: 0.7956, Precision: 0.7662, Recall: 0.8382, F1: 0.8006\n",
      "---⌛ Epoch 6/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095603f1e17a4429b3e0d170e978abe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eada0db3c2cc44f9a579c2ff066cc99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3872\n",
      "Validation Loss: 0.4274, Accuracy: 0.8036, Precision: 0.7911, Recall: 0.8137, F1: 0.8023\n",
      "---⌛ Epoch 7/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0679ea6ff44ca5a8dc4e6b92408d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211b757fc82a46869ccb1338ba133d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3684\n",
      "Validation Loss: 0.4246, Accuracy: 0.8044, Precision: 0.7724, Recall: 0.8513, F1: 0.8099\n",
      "---⌛ Epoch 8/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5c6efd4e9642fab3f654652f92a9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6270ab648f1743c9936ffbf9897425a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3572\n",
      "Validation Loss: 0.4160, Accuracy: 0.8072, Precision: 0.7845, Recall: 0.8358, F1: 0.8093\n",
      "---⌛ Epoch 9/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c1fbab3bc046998e294d9b05abf05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a57ce3d765747bcaee8769c5d931059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3362\n",
      "Validation Loss: 0.4163, Accuracy: 0.8124, Precision: 0.7862, Recall: 0.8472, F1: 0.8156\n",
      "---⌛ Epoch 10/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb011886b8540f1ad5c1db8dcb6af80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99250b4597634a31b26fe8765bdc8a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3198\n",
      "Validation Loss: 0.4432, Accuracy: 0.8084, Precision: 0.7592, Recall: 0.8913, F1: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 17:12:20 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/05/25 17:12:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b8d819290a43a1b24100458c7c37cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4330, Accuracy: 0.8160, Precision: 0.7820, Recall: 0.8811, F1: 0.8286\n",
      "\n",
      "Example Prediction on new data:\n",
      "Review: 'This movie was absolutely fantastic! I loved every moment of it.' -> Predicted Sentiment: positive\n",
      "Review: 'Utterly disappointing. A complete waste of time.' -> Predicted Sentiment: negative\n",
      "Review: 'It was okay, nothing special.' -> Predicted Sentiment: negative\n",
      "Review: 'The acting was superb, but the plot was a bit weak.' -> Predicted Sentiment: negative\n",
      "🏃 View run chapter03 at: http://localhost:80/#/experiments/1/runs/fd21aa738a3c4d108e54d6a68d5dc356\n",
      "🧪 View experiment at: http://localhost:80/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Training Loop ---\n",
    "\n",
    "num_epochs = 10 # As specified in PyTorch Lightning example\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id, run_name=\"chapter03\") as dl_model_tracking_run:\n",
    "    params = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"val_size\": len(val_dataset),\n",
    "        \"metric_function\" : \"accuracy\",\n",
    "        \"device\": str(device),\n",
    "        \"num_gpus\": torch.cuda.device_count(),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"max_token_length\": max_token_length\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "\n",
    "    with open(\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(summary(model)))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"---⌛ Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate_epoch(model, val_dataloader, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, \"\n",
    "                f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save the trained model to MLflow.\n",
    "    mlflow.pytorch.log_model(model, artifact_path=\"model\")\n",
    "\n",
    "    # --- 5. Testing ---\n",
    "    # Once the fine-tuning step is completed, we will test the accuracy\n",
    "    # of the model by running trainer.test():\n",
    "    print(\"\\nStarting testing...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1 = evaluate_epoch(model, test_dataloader, device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, \"\n",
    "            f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    # --- 6. Prediction Example ---\n",
    "    print(\"\\nExample Prediction on new data:\")\n",
    "    model.eval()\n",
    "    new_reviews = [\n",
    "        \"This movie was absolutely fantastic! I loved every moment of it.\",\n",
    "        \"Utterly disappointing. A complete waste of time.\",\n",
    "        \"It was okay, nothing special.\",\n",
    "        \"The acting was superb, but the plot was a bit weak.\"\n",
    "    ]\n",
    "\n",
    "    for review in new_reviews:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_token_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predicted_label_id = torch.argmax(logits, dim=-1).item()\n",
    "            predicted_sentiment = \"positive\" if predicted_label_id == 1 else \"negative\"\n",
    "            print(f\"Review: '{review}' -> Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: fd21aa738a3c4d108e54d6a68d5dc356; lifecycle_stage: active\n"
     ]
    }
   ],
   "source": [
    "run_id = dl_model_tracking_run.info.run_id\n",
    "print(\"run_id: {}; lifecycle_stage: {}\".format(run_id,\n",
    "    mlflow.get_run(run_id).info.lifecycle_stage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116cbb2a298c42abb99a67159add5886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: 'This movie was absolutely fantastic! I loved every moment of it.' -> Predicted Sentiment: positive\n",
      "Review: 'Utterly disappointing. A complete waste of time.' -> Predicted Sentiment: negative\n",
      "Review: 'It was okay, nothing special.' -> Predicted Sentiment: negative\n",
      "Review: 'The acting was superb, but the plot was a bit weak.' -> Predicted Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# use the run_id to construct a logged_model URI. An example is shown here:\n",
    "# logged_model = 'runs:/37a3fe9b6faf41d89001eca13ad6ca47/model'\n",
    "import mlflow.pytorch\n",
    "\n",
    "\n",
    "logged_model = f'runs:/{run_id}/model'\n",
    "\n",
    "# Load model as a pytorch model, not as the pyfunc model\n",
    "# model = mlflow.pytorch.load_model(logged_model)\n",
    "model = mlflow.pytorch.load_model(logged_model)\n",
    "\n",
    "new_reviews = [\n",
    "    \"This movie was absolutely fantastic! I loved every moment of it.\",\n",
    "    \"Utterly disappointing. A complete waste of time.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"The acting was superb, but the plot was a bit weak.\"\n",
    "]\n",
    "\n",
    "for review in new_reviews:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_token_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_label_id = torch.argmax(logits, dim=-1).item()\n",
    "        predicted_sentiment = \"positive\" if predicted_label_id == 1 else \"negative\"\n",
    "        print(f\"Review: '{review}' -> Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***MLFLOW.PYTORCH.LOAD_MODEL VERSUS MLFLOW.PYFUNC.LOAD_MODEL***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, and in the MLflow experiment tracking page's artifact section, if you have a logged model, MLflow will recommend using `mlflow.pyfunc.load_model` to load back a logged model for prediction. However, this only works for inputs such as a pandas DataFrame, NumPy array, or tensor; this does not work for an NLP text input. Since auto-logging for PyTorch lightning uses mlflow.`pytorch.log_model` to save the model, the correct way to load a logged model back is to use `mlflow.pytorch.load_model`, as we have shown here. This is because MLflow's default design is to use `mlflow.pyfunc.load_model` with standardization and a known limitation that can only accept input formats in terms of numbers. For text and image data, it requires a tokenization step as a preprocessing step. However, since the PyTorch model we saved here already performs tokenization as part of the serialized model, we can use the native `mlflow.pytorch.load_model` to directly load the model that accepts text as inputs.\n",
    "\n",
    "With that, we have successfully logged the model and loaded the model back to make a prediction. If we think this model is performing well enough, then we can register it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'nlp_dl_model'.\n",
      "2025/05/25 17:26:27 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: nlp_dl_model, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: nlp_dl_model\n",
      "Model Version: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'nlp_dl_model'.\n"
     ]
    }
   ],
   "source": [
    "# register the model\n",
    "model_registry_version = mlflow.register_model(logged_model, 'nlp_dl_model')\n",
    "print(f'Model Name: {model_registry_version.name}')\n",
    "print(f'Model Version: {model_registry_version.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<MLflow tracking server web UI showing the newly registered model.png>)\n",
    "\n",
    "**By default, a newly registered model's stage is `None`**, as shown in the preceding screenshot.\n",
    "\n",
    "By **having a model registered with a version number and stage label**, we have **laid the foundation for deployment to staging (also known as pre-production) and then production**. We will discuss how to perform model deployment based on registered models later.\n",
    "\n",
    "At this point, we have solved the two issues we raised at the beginning of this section regarding the limitations of auto-logging:\n",
    "\n",
    "* **How to load a logged DL PyTorch model** using the `mlflow.pytorch.load_model` API instead of the `mlflow.pyfunc.load_model` API\n",
    "* **How to register a logged DL PyTorch model** using the `mlflow.register_model` API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***CHOICES OF MLFLOW DL MODEL LOGGING APIS***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DL models, the **auto-logging for PyTorch only works for PyTorch lightning frameworks**. There are **other DL frameworks,** such as **TensorFlow, Keras, fastai, and MXNet**, that are also **supported by the corresponding MLflow auto-logging APIs**.\n",
    "\n",
    "For **other PyTorch frameworks such as Hugging Face, we can use MLflow's `mlflow.pyfunc.log_model` to log the model**, especially when we need to have multi-step DL model pipelines. We will implement such custom MLflow model flavors later in this book. If you don't want to use auto-logging for PyTorch, then you can directly use `mlflow.pytorch.log_model`. PyTorch's auto-logging uses `mlflow.pytorch.log_model` inside its implementation (see the official MLflow open source implementation [here](https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314)\n",
    "\n",
    "If we don't want to use auto-logging, then we can use MLflow's model logging API directly. This also gives us an alternative way to simultaneously register the model in one call. You can use the following line of code to both log and register the trained model:\n",
    "```py\n",
    "mlflow.pytorch.log_model(pytorch_model=trainer.model, artifact_path='dl_model', registered_model_name='nlp_dl_model')\n",
    "```\n",
    "Note that this line of code does not log any parameters or metrics of the model.\n",
    "\n",
    "With that, **we have not only logged many experiments and models in the tracking server for offline experimentation but also registered performant models for production deployment in the future with version control and provenance tracking.** We can now answer some of the provenance questions that we posted at the beginning of this chapter:\n",
    "\n",
    "\n",
    "The **why** and **where** provenance questions are yet to be fully answered but will be done so later in this book. This is because the **why** provenance question for the production model **can only be tracked and logged when the model is ready for deployment**, where **we need to add comments and reasons to justify the model's deployment**. The **where** provenance question can be answered fully **when we have a multiple-step model pipeline**. However, here, we only have a single-step pipeline, which is the simplest case. **A multi-step pipeline contains explicitly separate modulized code to specify which step performs what functionality so that we can easily change the detailed implementation of any of the steps without changing the flow of the pipeline**. In the next two sections, we will investigate how we can track metrics and the parameters of models without using auto-logging."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tracking model metrics**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default metric for the text classification model in the PyTorch `lightning-flash` package is **Accuracy**. If we want to change the metric to **F1 score** (a harmonic mean of precision and recall), which is a very common metric for measuring a classifier's performance, then we need to change the configuration of the classifier model before we start the model training process. Let's learn how to make this change and then use MLflow's non-auto-logging API to log the metrics:\n",
    "\n",
    "1. When defining the classifier variable, instead of using the default metric, we will pass a metric function called `torchmetrics.F1` as a variable, as follows:\n",
    "```py\n",
    "classifier_model = TextClassifier(backbone=\"prajjwal1/bert-tiny\", num_classes=datamodule.num_classes, metrics=torchmetrics.F1(datamodule.num_classes))\n",
    "```\n",
    "This uses the built-in metrics function of `torchmetrics`, the `F1` module, along with the number of classes in the data we need to classify as a parameter. This makes sure that the model is trained and tested using this new metric. You will see an output similar to the following:\n",
    "\n",
    "```py\n",
    "{'test_cross_entropy': 0.785443127155304, 'test_f1': 0.5343999862670898}\n",
    "```\n",
    "\n",
    "This shows that the model training and testing were using the F1 score as the metric, not the default accuracy metric. For more information on how you can use torchmetrics for customized metrics, please consult [its documentation site](https://torchmetrics.readthedocs.io/en/latest/).\n",
    "\n",
    "2. Now, if **we want to log all the metrics to the MLflow tracking server, including the training, validation, and testing metrics**, we need to get all the current metrics by calling the trainer's callback function, as follows:\n",
    "```py\n",
    "cur_metrics = trainer.callback_metrics\n",
    "```\n",
    "\n",
    "Then, we need to cast all the metric values to float to make sure that they are compatible with the MLflow log_metrics API:\n",
    "```python\n",
    "metrics = dict(map(lambda x: (x[0], float(x[1])), cur_metrics.items()))\n",
    "```\n",
    "\n",
    "3. Now, we can call MLflow's `log_metrics` to log all the metrics in the tracking server:\n",
    "```py    \n",
    "mlflow.log_metrics(metrics)\n",
    "```\n",
    "\n",
    "You will see the following metrics after using the F1 score as the classifier's metric, which will be logged in MLflow's tracking server:\n",
    "```py\n",
    "{\n",
    "   'train_f1': 0.5838666558265686,\n",
    "   'train_f1_step': 0.75,\n",
    "   'train_cross_entropy': 0.7465656399726868,\n",
    "   'train_cross_entropy_step': 0.30964696407318115,\n",
    "   'val_f1': 0.5203999876976013,\n",
    "   'val_cross_entropy': 0.8168156743049622,\n",
    "   'train_f1_epoch': 0.5838666558265686,\n",
    "   'train_cross_entropy_epoch': 0.7465656399726868,\n",
    "   'test_f1': 0.5343999862670898,\n",
    "   'test_cross_entropy': 0.785443127155304\n",
    "}\n",
    "```\n",
    "\n",
    "Using MLflow's `log_metrics` API **gives us more control with additional lines of code**, but **if we are satisfied with its auto-logging capabilities, then the only thing we need to change is what metric we want to use for the model training and testing processes**. In this case, we only need to define a new metric to use when declaring a new DL model (that is, use the F1 score instead of the default accuracy metric).\n",
    "\n",
    "4. If you want to **track multiple model metrics simultaneously, such as the F1 score, accuracy, precision, and recall, then the only thing you need to do is define a Python list of metrics you want to compute and track**, as follows:\n",
    "\n",
    "```python\n",
    "list_of_metrics = [\n",
    "   torchmetrics.Accuracy(),\n",
    "   torchmetrics.F1(num_classes=datamodule.num_classes),\n",
    "   torchmetrics.Precision(num_classes=datamodule.num_classes),\n",
    "   torchmetrics.Recall(num_classes=datamodule.num_classes)\n",
    "]\n",
    "```\n",
    "Then, in the model initialization statement, instead of passing a single metric to the metrics parameter, you can just pass the list_of_metrics Python list that we just defined, above the metrics parameter, as follows:\n",
    "```py\n",
    "classifier_model = TextClassifier(backbone=\"prajjwal1/bert-tiny\", num_classes=datamodule.num_classes, metrics=list_of_metrics)\n",
    "```\n",
    "No more changes need to be made to the rest of the code. So, in the `dl_model-non-auto-tracking.ipynb notebook` (https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb), you will notice that the preceding line is commented out by default. However, you can uncomment it and then comment out the previous one:\n",
    "```py\n",
    "classifier_model = TextClassifier(backbone=\"prajjwal1/bert-tiny\", num_classes=datamodule.num_classes, metrics=torchmetrics.F1Score(datamodule.num_classes))\n",
    "```\n",
    "\n",
    "Then, when you run the rest of the notebook, you will get the model testing reports, along with the following metrics, in the notebook's output:\n",
    "```python\n",
    "{\n",
    "   'test_accuracy': 0.6424000263214111,\n",
    "   'test_cross_entropy': 0.6315688490867615,\n",
    "   'test_f1': 0.6424000263214111,\n",
    "   'test_precision': 0.6424000263214111,\n",
    "   'test_recall': 0.6424000263214111\n",
    "}\n",
    "```\n",
    "**You may notice that the numbers for accuracy, F1, precision, and recall are the same. This is because, by default, torchmetrics uses a micro-average method, which computes a single scalar average score for all the classes by counting total true positives, false negatives, and false positives**. Scikit-learn has an average option called binary that outputs only the score for the positive label when it is a [binary classification model](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#).\n",
    "\n",
    "However, `torchmetrics` does not support a binary average method for a binary classification model. The only alternative is to use a none method, which computes the metric for each class and returns the metric for each class, even for a binary classification model. So, this does not produce a single scalar number. However, you can always call `scikit-learn`'s metrics API to compute an F1-score or other metrics based on the binary average method by passing two lists of values. Here, we can use `y_true` and `y_predict`, where `y_true` is the list of ground truth label values and y_predict is the list of model predicted label values. This can be a good exercise for you to try out as this is a common practice for all ML models, not special treatment for a DL model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tracking model parameters**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already seen, **there are lots of benefits of using auto-logging in MLflow**, but **if we want to track additional model parameters, we can either use MLflow to log additional parameters on top of what auto-logging records, or directly use MLflow to log all the parameters we want without using auto-logging at all**.\n",
    "\n",
    "Let's walk through a notebook without using MLflow auto-logging. If we want to have full control of what parameters will be logged by MLflow, we can use two APIs: `mlflow.log_param` and `mlflow.log_params`. The **first one logs a single pair of key-value parameters**, while **the second logs an entire dictionary of key-value parameters**. So, what kind of parameters might we be interested in tracking? The following answers this:\n",
    "\n",
    "* **Model hyperparameters**: Hyperparameters are defined before the learning process begins, which means they control how the learning process learns. These parameters can be turned and can directly affect how well a model trains. In a DL model, the list of hyperparameters includes the backbone language model, learning rate, loss function, the optimizer to be used, and many more. MLflow's auto-logging does not automatically log all the hyperparameters, so this is an opportunity for us to directly use MLflow's log_params API to record them in the experiment.\n",
    "* **Model parameters**: These parameters are learned during the model training process. For a DL model, these usually refer to the neural network weights that are learned during training. We don't need to log these weight parameters individually since they are already in the logged DL model.\n",
    "\n",
    "Let's log these hyperparameters using MLflow's log_params API, as follows:\n",
    "```py\n",
    " params = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"val_size\": len(val_dataset),\n",
    "        \"metric_function\" : \"accuracy\",\n",
    "        \"device\": str(device),\n",
    "        \"num_gpus\": torch.cuda.device_count(),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"max_token_length\": max_token_length\n",
    "    }\n",
    "```\n",
    "Note that here, we log the maximal number of epochs, the trainer's first optimizer's name, the optimizer's default parameters, and the overall classifier's hyperparameters (`classifier_model.hparams`). The one-line piece of code `mlflow.log_params(params)` logs all the key-value parameters in the params dictionary to the MLflow tracking server. If you see the following hyperparameters in the MLflow tracking server, then it means it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_params(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
