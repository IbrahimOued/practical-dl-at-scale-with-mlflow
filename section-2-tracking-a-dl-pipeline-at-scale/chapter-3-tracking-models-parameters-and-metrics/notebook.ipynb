{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Tracking Models, Parameters, and Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that MLflow can support multiple scenarios through the life cycle of DL models, it is common to use MLflow's capabilities incrementally. Usually, people start with MLflow tracking since it is easy to use and can handle many scenarios for reproducibility, provenance tracking, and auditing purposes.\n",
    "\n",
    "We will then take a deep dive into how we can track a model, along with its parameters and metrics, using MLflow's tracking and registry APIs. By the end of this chapter, you should feel comfortable using MLflow's tracking and registry APIs for various reproducibility and auditing purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up a full-fledged local MLflow tracking server**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of having a model registry is that we can register the model, version control the model, and prepare for model deployment into production. Therefore, this model registry will bridge the gap between offline experimentation and an online deployment production scenario. Thus, we need a full-fledged MLflow tracking server with the following stores to track the complete life cycle of a model:\n",
    "* **Backend store**: A **relational database backend is needed to support MLflow's storage of metadata** (metrics, parameters, and many others) about the experiment. This also allows the query capability of the experiment to be used. **We will use a MySQL database as a local backend store**.\n",
    "* **Artifact store**: An object store that can store arbitrary types of objects, such as serialized models, vocabulary files, figures, and many others. In a production environment, a popular choice is the AWS S3 store. We will use [**MinIO**](https://min.io/), a multi-cloud object store, as a local artifact store, which is fully compatible with the AWS S3 store API but can run on your laptop without you needing to access the cloud.\n",
    "\n",
    "To make this local setup as easy as possible, we will use the [docker-compose](https://docs.docker.com/compose/) tool with one line of command to start and stop a local full-fledged MLflow tracking server, as described in the following steps. The following steps will launch the local MLflow tracking server inside your local Docker container:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clone https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter03.\n",
    "2. Change directory to the `mlflow_docker_setup` subfolder, which can be found under the chapter03 folder.\n",
    "3. Run the following command:\n",
    "```bash\n",
    "bash start_mlflow.sh\n",
    "```\n",
    "4. Go to `http://localhost/` to see the MLflow UI web page. Then, click the Models tab in the UI. Note that this tab would not work if you only had a local filesystem as the backend store for the MLflow tracking server. Hence, the MLflow UI's backend is now running on the Docker container service you just started, not a local filesystem\n",
    "5. Go to `http://localhost:9000/`, and the following screen should appear for the **MinIO** artifact store web UI. Enter `minio` for Access Key and `minio123` for Secret Key. These are defined in the `.env` file, under the `mlflow_docker_setup` folder\n",
    "\n",
    "At this point, you should have a full-fledged local MLflow tracking server running successfully! If you want to stop the server, simply type the following command:\n",
    "```bash\n",
    "bash stop_mlflow.sh\n",
    "```\n",
    "The Docker-based MLflow tracking server will stop. We are now ready to use this local MLflow server to track model provenance, parameters, and metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tracking model provenance**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provenance** tracking for digital artifacts has been long studied in the litterature. For example, when **you're using a piece of patient diagnosis data in the biomedical industry**, people usually want to know **where it comes from, what kind of processing and cleaning has been done to the data, who owns the data, and other history and lineage information** about the data. **The rise of ML/DL models for industrial and business scenarios in production makes provenance tracking a required functionality**. The different granularities of provenance tracking are critical for operationalizing and managing not just the data science offline experimentation, but also before/during/after the model is deployed in production. So, what needs to be tracked for provenance?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the open provenance tracking framework**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a general provenance tracking framework to understand the big picture of why provenance tracking is a major effort. The following diagram is based on the [Open Provenance Model Vocabulary Specification](http://open-biomed.sourceforge.net/opmv/ns.html):\n",
    "\n",
    "![Text](open_provenance_model_vocab_spec.jpg)\n",
    "\n",
    "In the preceding diagram, there are three important items:\n",
    "\n",
    "* **Artifacts**: Things that are produced or used by processes (**A1** and **A2**).\n",
    "* **Processes**: Actions that are performed by using or producing artifacts (**P1** and **P2**).\n",
    "* **Causal relationships**: Edges or relationships between artifacts and processes, such as used, ***wasGeneratedBy***, and ***wasDerivedFrom*** in the preceding diagram (**R1**, **R2**, and **R3**).\n",
    "\n",
    "Intuitively, this open provenance model (OPM) framework allows us to ask the following 5W1H (five Ws and one H) questions, as follows:\n",
    "\n",
    "![Alt text](types_prov_questions.png)\n",
    "\n",
    "Having a systematic provenance framework and a set of questions will help us learn how to track model provenance and provide answers to these questions. This will motivate us when we implement MLflow model tracking in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementing MLflow model tracking**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use an MLflow tracking server to answer most of these types of provenance questions if we implement both **MLflow logging** and **registry** for the DL model we use. First, let's review what MLflow provides in terms of model provenance tracking. MLflow provides two sets of APIs for model provenance:\n",
    "* **Logging API**: This **allows each run of the experiment or a model pipeline to log** the model artifcat into the artifact store\n",
    "* **Registry API**: This **allows a centralized location to track the version** of the model and the stages of the model's life cycle (**None, Archived, Staging**, or **Production**)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***DIFFERENCE BETWEEN MODEL LOGGING AND MODEL REGISTRY***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although every run of the experiment needs to be logged and the model needs to be saved in the artifact store, **not every instance of the model needs to be registered in the model registry**. That's because, for many early exploratory model experimentations, the model might not be good. Thus, it is not necessarily registered to track the version. **Only when a model has good offline performance and becomes a candidate for promoting to production do we need to register it in the model registry to go through the model promotion process**.\n",
    "\n",
    "Although MLflow's official API documentation **separates logging and registry into two components**, we will **refer to them together as model tracking functionality in MLflow**.\n",
    "\n",
    "Although auto-logging is powerful, there are two issues with the current version:\n",
    "\n",
    "We already saw MLflow's auto-logging for the DL model build previously, although auto-logging is powerful, there are two issues with the current version:\n",
    "\n",
    "* It does not automatically register the model to the model registry.\n",
    "* It does not work out of the box for the logged model to work directly with the original input data (in our case, an English sentence) if you just follow MLflow's suggestion to use the `mlflow.pyfunc.load_model` API to load the logged model. This is a limitation that's probably due to the experimental nature of the current auto-logging APIs in MLflow.\n",
    "\n",
    "Let's walk through an example to review MLflow's capabilities and auto-logging's limitations and how we can solve them:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set up the following environment variables in your Bash terminal, where your MinIO and MySQL-based Docker component is running:\n",
    "```bash\n",
    "export MLFLOW_S3_ENDPOINT_URL=http://localhost:9000\n",
    "export AWS_ACCESS_KEY_ID=minio\n",
    "export AWS_SECRET_ACCESS_KEY=minio123\n",
    "```\n",
    "2. To follow along with this model tracking implementation, check out the dl_model_tracking.ipynb notebook file in VS Code by going to [this chapter's GitHub repository](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model_tracking.ipynb).\n",
    "\n",
    "ote that, in the fourth cell of the dl_model_tracking.ipynb notebook, we need to point it to the correct and new MLflow tracking URI that we just set up in the Docker and define a new experiment, as follows:\n",
    "```py\n",
    "EXPERIMENT_NAME = \"dl_model_chapter03\"\n",
    "mlflow.set_tracking_uri('http://localhost')\n",
    "```\n",
    "\n",
    "3. We will still use the auto-logging capabilities provided by MLflow but we will assign the run with a variable name, `dl_model_tracking_run`:\n",
    "`mlflow.pytorch.autolog()`\n",
    "```py\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id, run_name=\"chapter03\") as dl_model_tracking_run:\n",
    "    trainer.finetune(classifier_model, datamodule=datamodule, strategy=\"freeze\")\n",
    "    trainer.test(classifier_model, datamodule=datamodule)\n",
    "```\n",
    "`dl_model_tracking_run` allows us to get the `run_id` parameter and other metadata about this run programmatically, as we will see in the next step. Once this code cell has been executed, we will have a trained model logged in the MLflow tracking server with all the required parameters and metrics. However, the model hasn't been registered yet. We can find the logged experiment in the MLflow web UI, along with all the relevant parameters and metrics, at http://localhost/#/experiments/1/runs/37a3fe9b6faf41d89001eca13ad6ca47. You can find the model artifacts in the MinIO storage backend. Go to http://localhost:9000/minio/mlflow/1/37a3fe9b6faf41d89001eca13ad6ca47/artifacts/model/ to see the storage UI.\n",
    "4. Retrieve the run_id parameter from dl_model_tracking_run, as well as other metadata, as follows:\n",
    "```py\n",
    "run_id = dl_model_tracking_run.info.run_id\n",
    "print(\"run_id: {}; lifecycle_stage: {}\".format(run_id,\n",
    "    mlflow.get_run(run_id).info.lifecycle_stage))\n",
    "```\n",
    "This will print out something like the following:\n",
    "```bash\n",
    "run_id: 37a3fe9b6faf41d89001eca13ad6ca47; lifecycle_stage: active\n",
    "```\n",
    "5. Retrieve the logged model by defining the logged model URI. This will allow us to reload the logged model at this specific location:\n",
    "```py\n",
    "logged_model = f'runs:/{run_id}/model'\n",
    "```\n",
    "6. se mlflow.pytorch.load_model and the following logged_model URI to load the model back into memory and make a new prediction for a given input sentence, as follows:\n",
    "```py\n",
    "model = mlflow.pytorch.load_model(logged_model)\n",
    "model.predict({'This is great news'})\n",
    "```\n",
    "This will output a model prediction label, as follows:\n",
    "```bash\n",
    "['positive']\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import torch\n",
    "from flash.core.data.utils import download_data\n",
    "from flash.text import TextClassificationData, TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://localhost:9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(\"https://pl-flash-data.s3.amazonaws.com/imdb.zip\", \"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22500/22500 [00:04<00:00, 4747.30ex/s]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 5500.37ex/s]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 4092.03ex/s]\n"
     ]
    }
   ],
   "source": [
    "datamodule = TextClassificationData.from_csv(\n",
    "    \"review\",\n",
    "    \"sentiment\",\n",
    "    train_file=\"data/imdb/train.csv\",\n",
    "    val_file=\"data/imdb/valid.csv\",\n",
    "    test_file=\"data/imdb/test.csv\",\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 'prajjwal1/bert-tiny' provided by Hugging Face/transformers (https://github.com/huggingface/transformers).\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from flash import Trainer\n",
    "classifier_model = TextClassifier(backbone=\"prajjwal1/bert-tiny\", num_classes=datamodule.num_classes)\n",
    "trainer = Trainer(max_epochs=3, gpus=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Cannot set a deleted experiment 'dl_model_chapter03' as the active experiment. You can restore the experiment, or permanently delete the experiment to create a new one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m EXPERIMENT_NAME \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdl_model_chapter03\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m mlflow\u001b[39m.\u001b[39mset_tracking_uri(\u001b[39m'\u001b[39m\u001b[39mhttp://127.0.0.1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m mlflow\u001b[39m.\u001b[39;49mset_experiment(EXPERIMENT_NAME)\n\u001b[1;32m      4\u001b[0m experiment \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39mget_experiment_by_name(EXPERIMENT_NAME)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mexperiment_id:\u001b[39m\u001b[39m\"\u001b[39m, experiment\u001b[39m.\u001b[39mexperiment_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow/lib/python3.10/site-packages/mlflow/tracking/fluent.py:132\u001b[0m, in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    127\u001b[0m             message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExperiment with ID \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mexperiment_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    128\u001b[0m             error_code\u001b[39m=\u001b[39mRESOURCE_DOES_NOT_EXIST,\n\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m experiment\u001b[39m.\u001b[39mlifecycle_stage \u001b[39m!=\u001b[39m LifecycleStage\u001b[39m.\u001b[39mACTIVE:\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    133\u001b[0m         message\u001b[39m=\u001b[39m(\n\u001b[1;32m    134\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot set a deleted experiment \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m as the active experiment. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou can restore the experiment, or permanently delete the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexperiment to create a new one.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m experiment\u001b[39m.\u001b[39mname\n\u001b[1;32m    137\u001b[0m         ),\n\u001b[1;32m    138\u001b[0m         error_code\u001b[39m=\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[39mglobal\u001b[39;00m _active_experiment_id\n\u001b[1;32m    142\u001b[0m _active_experiment_id \u001b[39m=\u001b[39m experiment\u001b[39m.\u001b[39mexperiment_id\n",
      "\u001b[0;31mMlflowException\u001b[0m: Cannot set a deleted experiment 'dl_model_chapter03' as the active experiment. You can restore the experiment, or permanently delete the experiment to create a new one."
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"dl_model_chapter03\"\n",
    "mlflow.set_tracking_uri('http://127.0.0.1')\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "print(\"experiment_id:\", experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/01/11 21:29:50 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | train_metrics | ModuleDict         | 0     \n",
      "1 | val_metrics   | ModuleDict         | 0     \n",
      "2 | test_metrics  | ModuleDict         | 0     \n",
      "3 | adapter       | HuggingFaceAdapter | 4.4 M \n",
      "-----------------------------------------------------\n",
      "258       Trainable params\n",
      "4.4 M     Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.545    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 313/313 [00:15<00:00, 20.66it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.6015999913215637\n",
      "   test_cross_entropy       0.6579326391220093\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "mlflow.pytorch.autolog()\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id, run_name=\"chapter03\") as dl_model_tracking_run:\n",
    "    trainer.finetune(classifier_model, datamodule=datamodule, strategy=\"freeze\")\n",
    "    trainer.test(classifier_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 7a82a1f48277411ca448006317b94143; lifecycle_stage: active\n"
     ]
    }
   ],
   "source": [
    "run_id = dl_model_tracking_run.info.run_id\n",
    "print(\"run_id: {}; lifecycle_stage: {}\".format(run_id,\n",
    "    mlflow.get_run(run_id).info.lifecycle_stage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the run_id to construct a logged_model URI. An example is shown here:\n",
    "# logged_model = 'runs:/37a3fe9b6faf41d89001eca13ad6ca47/model'\n",
    "logged_model = f'runs:/{run_id}/model'\n",
    "\n",
    "# Load model as a pytorch model, not as the pyfunc model\n",
    "model = mlflow.pytorch.load_model(logged_model)\n",
    "testdatamodule = TextClassificationData.from_lists(\n",
    "    predict_data=[\n",
    "        \"What a piece of disappointing news?\",\n",
    "        \"The best movie in the history of cinema.\",\n",
    "        \"This is great news\",\n",
    "    ],\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "trainer.predict(model, dataloaders=testdatamodule, output='labels')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***MLFLOW.PYTORCH.LOAD_MODEL VERSUS MLFLOW.PYFUNC.LOAD_MODEL***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, and in the MLflow experiment tracking page's artifact section, if you have a logged model, MLflow will recommend using `mlflow.pyfunc.load_model` to load back a logged model for prediction. However, this only works for inputs such as a pandas DataFrame, NumPy array, or tensor; this does not work for an NLP text input. Since auto-logging for PyTorch lightning uses mlflow.`pytorch.log_model` to save the model, the correct way to load a logged model back is to use `mlflow.pytorch.load_model`, as we have shown here. This is because MLflow's default design is to use `mlflow.pyfunc.load_model` with standardization and a known limitation that can only accept input formats in terms of numbers. For text and image data, it requires a tokenization step as a preprocessing step. However, since the PyTorch model we saved here already performs tokenization as part of the serialized model, we can use the native `mlflow.pytorch.load_model` to directly load the model that accepts text as inputs.\n",
    "\n",
    "With that, we have successfully logged the model and loaded the model back to make a prediction. If we think this model is performing well enough, then we can register it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'nlp_dl_model' already exists. Creating a new version of this model...\n",
      "2023/01/11 23:35:52 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: nlp_dl_model, version 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: nlp_dl_model\n",
      "Model Version: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'nlp_dl_model'.\n"
     ]
    }
   ],
   "source": [
    "# register the model\n",
    "model_registry_version = mlflow.register_model(logged_model, 'nlp_dl_model')\n",
    "print(f'Model Name: {model_registry_version.name}')\n",
    "print(f'Model Version: {model_registry_version.version}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](MLflow%20tracking%20server%20web%20UI%20showing%20the%20newly%20registered%20model.png)\n",
    "\n",
    "**By default, a newly registered model's stage is `None`**, as shown in the preceding screenshot.\n",
    "\n",
    "By **having a model registered with a version number and stage label**, we have **laid the foundation for deployment to staging (also known as pre-production) and then production**. We will discuss how to perform model deployment based on registered models later.\n",
    "\n",
    "At this point, we have solved the two issues we raised at the beginning of this section regarding the limitations of auto-logging:\n",
    "\n",
    "* **How to load a logged DL PyTorch model** using the `mlflow.pytorch.load_model` API instead of the `mlflow.pyfunc.load_model` API\n",
    "* **How to register a logged DL PyTorch model** using the `mlflow.register_model` API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***CHOICES OF MLFLOW DL MODEL LOGGING APIS***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DL models, the **auto-logging for PyTorch only works for PyTorch lightning frameworks**. There are **other DL frameworks,** such as **TensorFlow, Keras, fastai, and MXNet**, that are also **supported by the corresponding MLflow auto-logging APIs**. For o**ther PyTorch frameworks such as Hugging Face, we can use MLflow's `mlflow.pyfunc.log_model` to log the model**, especially when we need to have multi-step DL model pipelines. We will implement such custom MLflow model flavors later in this book. If you don't want to use auto-logging for PyTorch, then you can directly use `mlflow.pytorch.log_model`. PyTorch's auto-logging uses `mlflow.pytorch.log_model` inside its implementation (see the official MLflow open source implementation [here](https://github.com/mlflow/mlflow/blob/290bf3d54d1e5ce61944455cb302a5d6390107f0/mlflow/pytorch/_pytorch_autolog.py#L314)\n",
    "\n",
    "If we don't want to use auto-logging, then we can use MLflow's model logging API directly. This also gives us an alternative way to simultaneously register the model in one call. You can use the following line of code to both log and register the trained model:\n",
    "```py\n",
    "mlflow.pytorch.log_model(pytorch_model=trainer.model, artifact_path='dl_model', registered_model_name='nlp_dl_model')\n",
    "```\n",
    "Note that this line of code does not log any parameters or metrics of the model.\n",
    "\n",
    "With that, **we have not only logged many experiments and models in the tracking server for offline experimentation but also registered performant models for production deployment in the future with version control and provenance tracking.** We can now answer some of the provenance questions that we posted at the beginning of this chapter:\n",
    "\n",
    "\n",
    "The **why** and **where** provenance questions are yet to be fully answered but will be done so later in this book. This is because the **why** provenance question for the production model **can only be tracked and logged when the model is ready for deployment**, where **we need to add comments and reasons to justify the model's deployment**. The **where** provenance question can be answered fully **when we have a multiple-step model pipeline**. However, here, we only have a single-step pipeline, which is the simplest case. **A multi-step pipeline contains explicitly separate modulized code to specify which step performs what functionality so that we can easily change the detailed implementation of any of the steps without changing the flow of the pipeline**. In the next two sections, we will investigate how we can track metrics and the parameters of models without using auto-logging."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tracking model metrics**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default metric for the text classification model in the PyTorch `lightning-flash` package is **Accuracy**. If we want to change the metric to **F1 score** (a harmonic mean of precision and recall), which is a very common metric for measuring a classifier's performance, then we need to change the configuration of the classifier model before we start the model training process. Let's learn how to make this change and then use MLflow's non-auto-logging API to log the metrics:\n",
    "\n",
    "1. When defining the classifier variable, instead of using the default metric, we will pass a metric function called `torchmetrics.F1` as a variable, as follows:\n",
    "```py\n",
    "classifier_model = TextClassifier(backbone=\"prajjwal1/bert-tiny\", num_classes=datamodule.num_classes, metrics=torchmetrics.F1(datamodule.num_classes))\n",
    "```\n",
    "This uses the built-in metrics function of `torchmetrics`, the `F1` module, along with the number of classes in the data we need to classify as a parameter. This makes sure that the model is trained and tested using this new metric. You will see an output similar to the following:\n",
    "\n",
    "```py\n",
    "{'test_cross_entropy': 0.785443127155304, 'test_f1': 0.5343999862670898}\n",
    "```\n",
    "\n",
    "This shows that the model training and testing were using the F1 score as the metric, not the default accuracy metric. For more information on how you can use torchmetrics for customized metrics, please consult [its documentation site](https://torchmetrics.readthedocs.io/en/latest/).\n",
    "\n",
    "2. Now, if **we want to log all the metrics to the MLflow tracking server, including the training, validation, and testing metrics**, we need to get all the current metrics by calling the trainer's callback function, as follows:\n",
    "```py\n",
    "cur_metrics = trainer.callback_metrics\n",
    "```\n",
    "\n",
    "Then, we need to cast all the metric values to float to make sure that they are compatible with the MLflow log_metrics API:\n",
    "```python\n",
    "metrics = dict(map(lambda x: (x[0], float(x[1])), cur_metrics.items()))\n",
    "```\n",
    "\n",
    "3. Now, we can call MLflow's `log_metrics` to log all the metrics in the tracking server:\n",
    "```py    \n",
    "mlflow.log_metrics(metrics)\n",
    "```\n",
    "\n",
    "You will see the following metrics after using the F1 score as the classifier's metric, which will be logged in MLflow's tracking server:\n",
    "```py\n",
    "{\n",
    "   'train_f1': 0.5838666558265686,\n",
    "   'train_f1_step': 0.75,\n",
    "   'train_cross_entropy': 0.7465656399726868,\n",
    "   'train_cross_entropy_step': 0.30964696407318115,\n",
    "   'val_f1': 0.5203999876976013,\n",
    "   'val_cross_entropy': 0.8168156743049622,\n",
    "   'train_f1_epoch': 0.5838666558265686,\n",
    "   'train_cross_entropy_epoch': 0.7465656399726868,\n",
    "   'test_f1': 0.5343999862670898,\n",
    "   'test_cross_entropy': 0.785443127155304\n",
    "}\n",
    "```\n",
    "\n",
    "Using MLflow's `log_metrics` API **gives us more control with additional lines of code**, but **if we are satisfied with its auto-logging capabilities, then the only thing we need to change is what metric we want to use for the model training and testing processes**. In this case, we only need to define a new metric to use when declaring a new DL model (that is, use the F1 score instead of the default accuracy metric).\n",
    "\n",
    "4. If you want to **track multiple model metrics simultaneously, such as the F1 score, accuracy, precision, and recall, then the only thing you need to do is define a Python list of metrics you want to compute and track**, as follows:\n",
    "\n",
    "```python\n",
    "list_of_metrics = [\n",
    "   torchmetrics.Accuracy(),\n",
    "   torchmetrics.F1(num_classes=datamodule.num_classes),\n",
    "   torchmetrics.Precision(num_classes=datamodule.num_classes),\n",
    "   torchmetrics.Recall(num_classes=datamodule.num_classes)\n",
    "]\n",
    "```\n",
    "Then, in the model initialization statement, instead of passing a single metric to the metrics parameter, you can just pass the list_of_metrics Python list that we just defined, above the metrics parameter, as follows:\n",
    "```py\n",
    "classifier_model = TextClassifier(backbone=\"prajjwal1/bert-tiny\", num_classes=datamodule.num_classes, metrics=list_of_metrics)\n",
    "```\n",
    "No more changes need to be made to the rest of the code. So, in the `dl_model-non-auto-tracking.ipynb notebook` (https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter03/dl_model-non-auto-tracking.ipynb), you will notice that the preceding line is commented out by default. However, you can uncomment it and then comment out the previous one:\n",
    "```py\n",
    "classifier_model = TextClassifier(backbone=\"prajjwal1/bert-tiny\", num_classes=datamodule.num_classes, metrics=torchmetrics.F1(datamodule.num_classes))\n",
    "```\n",
    "\n",
    "Then, when you run the rest of the notebook, you will get the model testing reports, along with the following metrics, in the notebook's output:\n",
    "```python\n",
    "{\n",
    "   'test_accuracy': 0.6424000263214111,\n",
    "   'test_cross_entropy': 0.6315688490867615,\n",
    "   'test_f1': 0.6424000263214111,\n",
    "   'test_precision': 0.6424000263214111,\n",
    "   'test_recall': 0.6424000263214111\n",
    "}\n",
    "```\n",
    "Y**ou may notice that the numbers for accuracy, F1, precision, and recall are the same. This is because, by default, torchmetrics uses a micro-average method, which computes a single scalar average score for all the classes by counting total true positives, false negatives, and false positives**. Scikit-learn has an average option called binary that outputs only the score for the positive label when it is a [binary classification model](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#).\n",
    "\n",
    "However, `torchmetrics` does not support a binary average method for a binary classification model. The only alternative is to use a none method, which computes the metric for each class and returns the metric for each class, even for a binary classification model. So, this does not produce a single scalar number. However, you can always call `scikit-learn`'s metrics API to compute an F1-score or other metrics based on the binary average method by passing two lists of values. Here, we can use `y_true` and `y_predict`, where `y_true` is the list of ground truth label values and y_predict is the list of model predicted label values. This can be a good exercise for you to try out as this is a common practice for all ML models, not special treatment for a DL model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tracking model parameters**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already seen, **there are lots of benefits of using auto-logging in MLflow**, but **if we want to track additional model parameters, we can either use MLflow to log additional parameters on top of what auto-logging records, or directly use MLflow to log all the parameters we want without using auto-logging at all**.\n",
    "\n",
    "Let's walk through a notebook without using MLflow auto-logging. If we want to have full control of what parameters will be logged by MLflow, we can use two APIs: `mlflow.log_param` and `mlflow.log_params`. The **first one logs a single pair of key-value parameters**, while **the second logs an entire dictionary of key-value parameters**. So, what kind of parameters might we be interested in tracking? The following answers this:\n",
    "\n",
    "* **Model hyperparameters**: Hyperparameters are defined before the learning process begins, which means they control how the learning process learns. These parameters can be turned and can directly affect how well a model trains. In a DL model, the list of hyperparameters includes the backbone language model, learning rate, loss function, the optimizer to be used, and many more. MLflow's auto-logging does not automatically log all the hyperparameters, so this is an opportunity for us to directly use MLflow's log_params API to record them in the experiment.\n",
    "* **Model parameters**: These parameters are learned during the model training process. For a DL model, these usually refer to the neural network weights that are learned during training. We don't need to log these weight parameters individually since they are already in the logged DL model.\n",
    "\n",
    "Let's log these hyperparameters using MLflow's log_params API, as follows:\n",
    "```PY\n",
    "params = {\"epochs\": trainer.max_epochs}\n",
    "if hasattr(trainer, \"optimizers\"):\n",
    "    optimizer = trainer.optimizers[0]\n",
    "    params[\"optimizer_name\"] = optimizer.__class__.__name__\n",
    "if hasattr(optimizer, \"defaults\"):\n",
    "    params.update(optimizer.defaults)\n",
    "params.update(classifier_model.hparams)\n",
    "mlflow.log_params(params)\n",
    "```\n",
    "Note that here, we log the maximal number of epochs, the trainer's first optimizer's name, the optimizer's default parameters, and the overall classifier's hyperparameters (`classifier_model.hparams`). The one-line piece of code `mlflow.log_params(params)` logs all the key-value parameters in the params dictionary to the MLflow tracking server. If you see the following hyperparameters in the MLflow tracking server, then it means it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RestException",
     "evalue": "INVALID_PARAMETER_VALUE: The run f964bcce1d404e79a33cecc4edeca04c must be in the 'active' state. Current state is deleted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     params\u001b[39m.\u001b[39mupdate(optimizer\u001b[39m.\u001b[39mdefaults)\n\u001b[1;32m      7\u001b[0m params\u001b[39m.\u001b[39mupdate(classifier_model\u001b[39m.\u001b[39mhparams)\n\u001b[0;32m----> 8\u001b[0m mlflow\u001b[39m.\u001b[39;49mlog_params(params)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow/lib/python3.10/site-packages/mlflow/tracking/fluent.py:699\u001b[0m, in \u001b[0;36mlog_params\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    697\u001b[0m run_id \u001b[39m=\u001b[39m _get_or_start_run()\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[1;32m    698\u001b[0m params_arr \u001b[39m=\u001b[39m [Param(key, \u001b[39mstr\u001b[39m(value)) \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m--> 699\u001b[0m MlflowClient()\u001b[39m.\u001b[39;49mlog_batch(run_id\u001b[39m=\u001b[39;49mrun_id, metrics\u001b[39m=\u001b[39;49m[], params\u001b[39m=\u001b[39;49mparams_arr, tags\u001b[39m=\u001b[39;49m[])\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow/lib/python3.10/site-packages/mlflow/tracking/client.py:965\u001b[0m, in \u001b[0;36mMlflowClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_batch\u001b[39m(\n\u001b[1;32m    909\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    910\u001b[0m     run_id: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m     tags: Sequence[RunTag] \u001b[39m=\u001b[39m (),\n\u001b[1;32m    914\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[39m    Log multiple metrics, params, and/or tags.\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39m        status: FINISHED\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 965\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mlog_batch(run_id, metrics, params, tags)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:389\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[1;32m    386\u001b[0m     metrics_batch \u001b[39m=\u001b[39m metrics[:metrics_batch_size]\n\u001b[1;32m    387\u001b[0m     metrics \u001b[39m=\u001b[39m metrics[metrics_batch_size:]\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mlog_batch(\n\u001b[1;32m    390\u001b[0m         run_id\u001b[39m=\u001b[39;49mrun_id, metrics\u001b[39m=\u001b[39;49mmetrics_batch, params\u001b[39m=\u001b[39;49mparams_batch, tags\u001b[39m=\u001b[39;49mtags_batch\n\u001b[1;32m    391\u001b[0m     )\n\u001b[1;32m    393\u001b[0m \u001b[39mfor\u001b[39;00m metrics_batch \u001b[39min\u001b[39;00m chunk_list(metrics, chunk_size\u001b[39m=\u001b[39mMAX_METRICS_PER_BATCH):\n\u001b[1;32m    394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mlog_batch(run_id\u001b[39m=\u001b[39mrun_id, metrics\u001b[39m=\u001b[39mmetrics_batch, params\u001b[39m=\u001b[39m[], tags\u001b[39m=\u001b[39m[])\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:321\u001b[0m, in \u001b[0;36mRestStore.log_batch\u001b[0;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[1;32m    317\u001b[0m tag_protos \u001b[39m=\u001b[39m [tag\u001b[39m.\u001b[39mto_proto() \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tags]\n\u001b[1;32m    318\u001b[0m req_body \u001b[39m=\u001b[39m message_to_json(\n\u001b[1;32m    319\u001b[0m     LogBatch(metrics\u001b[39m=\u001b[39mmetric_protos, params\u001b[39m=\u001b[39mparam_protos, tags\u001b[39m=\u001b[39mtag_protos, run_id\u001b[39m=\u001b[39mrun_id)\n\u001b[1;32m    320\u001b[0m )\n\u001b[0;32m--> 321\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(LogBatch, req_body)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:56\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body)\u001b[0m\n\u001b[1;32m     54\u001b[0m endpoint, method \u001b[39m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m     55\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:281\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     response \u001b[39m=\u001b[39m http_request(\n\u001b[1;32m    279\u001b[0m         host_creds\u001b[39m=\u001b[39mhost_creds, endpoint\u001b[39m=\u001b[39mendpoint, method\u001b[39m=\u001b[39mmethod, json\u001b[39m=\u001b[39mjson_body\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 281\u001b[0m response \u001b[39m=\u001b[39m verify_rest_response(response, endpoint)\n\u001b[1;32m    282\u001b[0m js_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext)\n\u001b[1;32m    283\u001b[0m parse_dict(js_dict\u001b[39m=\u001b[39mjs_dict, message\u001b[39m=\u001b[39mresponse_proto)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlflow/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:207\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m _can_parse_as_json_object(response\u001b[39m.\u001b[39mtext):\n\u001b[0;32m--> 207\u001b[0m         \u001b[39mraise\u001b[39;00m RestException(json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext))\n\u001b[1;32m    208\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         base_msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAPI request to endpoint \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m failed with error code \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m != 200\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    210\u001b[0m             endpoint,\n\u001b[1;32m    211\u001b[0m             response\u001b[39m.\u001b[39mstatus_code,\n\u001b[1;32m    212\u001b[0m         )\n",
      "\u001b[0;31mRestException\u001b[0m: INVALID_PARAMETER_VALUE: The run f964bcce1d404e79a33cecc4edeca04c must be in the 'active' state. Current state is deleted."
     ]
    }
   ],
   "source": [
    "params = {\"epochs\": trainer.max_epochs}\n",
    "if hasattr(trainer, \"optimizers\"):\n",
    "    optimizer = trainer.optimizers[0]\n",
    "    params[\"optimizer_name\"] = optimizer.__class__.__name__\n",
    "if hasattr(optimizer, \"defaults\"):\n",
    "    params.update(optimizer.defaults)\n",
    "params.update(classifier_model.hparams)\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccdc0736e01b132d97e78ee7b194f8632625391a8773c1a4eedba6ee981c132b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
