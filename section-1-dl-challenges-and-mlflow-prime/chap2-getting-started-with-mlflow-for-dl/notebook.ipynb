{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 : Getting started with MLflow for DL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up MLflow**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up MLflow to interact with a remote MLflow server**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a corporate production environment, MLflow is usually hosted on a cloud server, which could be self-hosted or one of the Databricks' managed services in one of the cloud providers (such as AWS, Azure, or Google Cloud). In those cases, there is a requirement to set up your local development environment so that you can run your ML/DL experiment locally but interact with the MLflow server remotely. Next, we will describe how to do this using environment variables with the help of the following three steps:\n",
    "1. In a bash shell command-line environment, define three new environment variables if you are using a Databricks-managed MLflow tracking server. The first environment variable is `MLFLOW_TRACKING_URI`, and the assigned value is databricks:\n",
    "```bash\n",
    "export MLFLOW_TRACKING_URI=databricks\n",
    "export DATABRICKS_HOST=https://*******\n",
    "export DATABRICKS_TOKEN=dapi******\n",
    "```\n",
    "2. The second environment variable is `DATABRICKS_HOST`. If your Databricks managed website looks like `https://dbc-*.cloud.databricks.com/`, then that's the value of the `DATABRICKS_HOST` variable (replace `*` with your actual website string).\n",
    "3. The third environment variable is `DATABRICKS_TOKEN`. Navigate to your Databricks-managed website at `https://dbc-*.cloud.databricks.com/#setting/account`, click on Access Tokens, and then click on Generate New Token. You will see a pop-up window with a Comment field (which can be used to record why this token will be used) and expiration date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementing our first DL experiment with MLflow autologging**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the DL sentiment classifier we built previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 10:56:08 INFO mlflow.tracking.fluent: Experiment with name 'dl_model_chapter2' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_id: 239468158995223302\n"
     ]
    }
   ],
   "source": [
    "# 1 First we need to import the MLflow module\n",
    "import mlflow\n",
    "# This will provide API for logging and loading models\n",
    "# 2 Just before we run the training code, we need to set up an active experiment using\n",
    "# mlflow.set_experiment for the current running code\n",
    "EXPERIMENT_NAME = \"dl_model_chapter2\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "print(\"experiment_id:\", experiment.experiment_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets an experiment named `dl_model_chapter02` to be the current active experiment. If this experiment does not exist in your current tracking server, it will be created automatically.\n",
    "\n",
    "#### **ENVIRONMENT VARIABLE**\n",
    "\n",
    "*Note that you might need to set the tracking server URI using the MLFLOW_TRACKING_URI environment variable before you run your first experiment. If you are using a hosted Databricks server, implement the following*:\n",
    "```bash\n",
    "export MLFLOW_TRACKING_URI=databricks\n",
    "```\n",
    "\n",
    "*If you are using a local server, then set this environment variable to empty or the default localhost at port number 5000 as follows (note that this is our current section's scenario and assumes you are using a local server):*\n",
    "```bash\n",
    "export MLFLOW_TRACKING_URI= http://127.0.0.1:5000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export MLFLOW_TRACKING_URI=http://127.0.0.1:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/25 10:56:14 WARNING mlflow.utils.autologging_utils: MLflow pytorch autologging is known to be compatible with 1.9.0 <= torch <= 2.6.0, but the installed version is 2.7.0+cu126. If you encounter errors during autologging, try upgrading / downgrading torch to a compatible version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "# 3 Next, add one line of code to enable autologging in MLflow:\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will allow the default parameters, metrics, and model to be automatically logged to the MLflow tracking server."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AUTOLOGGING IN MLFLOW**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Autologging in MLflow is still in experiment mode (as of version 1.20.2) and might change in the future. Here, we use it to explore the MLflow components since it only requires one line of code to automatically log everything of interest. In the upcoming chapters, we will learn about and implement additional ways to perform tracking and logging in MLflow. Also, note that currently, autologging in MLflow for PyTorch (as of version 1.20.2) only works for the PyTorch Lightning framework, not any arbitrary PyTorch code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# --- 1. Data Download Utility (similar to Flash's download_data) ---\n",
    "def download_and_extract_zip(url, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    zip_file_name = os.path.join(path, \"downloaded_data.zip\")\n",
    "\n",
    "    print(f\"Downloading data from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 # 1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(zip_file_name, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong during download\")\n",
    "        return\n",
    "\n",
    "    print(f\"Extracting {zip_file_name} to {path}...\")\n",
    "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path)\n",
    "    os.remove(zip_file_name) # Clean up the zip file\n",
    "    print(\"Download and extraction complete.\")\n",
    "\n",
    "# --- 2. Custom Dataset for IMDB (replaces TextClassificationData) ---\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Ensure 'sentiment' is mapped to numerical labels\n",
    "        # 0 for negative, 1 for positive\n",
    "        self.label_map = {'negative': 0, 'positive': 1}\n",
    "        self.dataframe['sentiment_label'] = self.dataframe['sentiment'].map(self.label_map)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.dataframe.loc[idx, 'review'])\n",
    "        label = self.dataframe.loc[idx, 'sentiment_label']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt', # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Training Function ---\n",
    "# \"\"\"\n",
    "# Once we have the data, we can now perform fine-tuning using a foundation model.\n",
    "# First, we declare classifier_model by calling TextClassifier with a backbone\n",
    "# assigned to prajjwal1/bert-tiny (which is a much smaller BERT-like pretrained\n",
    "# model located in the Hugging Face model repository: https://huggingface.co/prajjwal1/bert-tiny).\n",
    "# This means our model will be based on the bert-tiny model.\n",
    "# \"\"\"\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# --- 4. Evaluation Function ---\n",
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    # Get precision, recall, f1-score for positive class (label 1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true_labels, all_predictions, average='binary', pos_label=1\n",
    "    )\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://pl-flash-data.s3.amazonaws.com/imdb.zip...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ad7adad4014e3aa24c98975b5b3280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/15.9M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/downloaded_data.zip to ./data/...\n",
      "Download and extraction complete.\n",
      "Number of classes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 1. & 2. Data Download and Preparation ---\n",
    "data_url = \"https://pl-flash-data.s3.amazonaws.com/imdb.zip\"\n",
    "data_path = \"./data/\"\n",
    "download_and_extract_zip(data_url, data_path)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_path, \"imdb/train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(data_path, \"imdb/valid.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_path, \"imdb/test.csv\"))\n",
    "\n",
    "# Determine number of classes from the sentiment column\n",
    "num_classes = train_df['sentiment'].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Initialize Tokenizer\n",
    "model_name = \"prajjwal1/bert-tiny\" # As used in Flash example\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_token_length = 128 # A common max length for BERT-tiny, adjust as needed\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = IMDBDataset(train_df, tokenizer, max_token_length)\n",
    "val_dataset = IMDBDataset(val_df, tokenizer, max_token_length)\n",
    "test_dataset = IMDBDataset(test_df, tokenizer, max_token_length)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# --- 3. Model Definition and Setup ---\n",
    "# Use AutoModelForSequenceClassification for classification tasks with Hugging Face models\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Learning Rate\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # Common learning rate for fine-tuning BERT\n",
    "\n",
    "# --- 4. Training Loop ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: b1355d743ccb49d3b362a521ce65c826\n",
      "\n",
      "Starting training...\n",
      "---⌛ Epoch 1/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02fafc54ded4eaab5546c1debf794a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 10 # As specified in PyTorch Lightning example\n",
    "learning_rate = 2e-5 # Common learning rate for fine-tuning BERT\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    params = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"val_size\": len(val_dataset),\n",
    "        \"metric_function\" : \"accuracy\",\n",
    "        \"device\": str(device),\n",
    "        \"num_gpus\": torch.cuda.device_count(),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_token_length\": max_token_length,\n",
    "        \"optimizer\": type(optimizer).__name__,\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "\n",
    "    with open(\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(summary(model)))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"---⌛ Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate_epoch(model, val_dataloader, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, \"\n",
    "                f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save the trained model to MLflow.\n",
    "    mlflow.pytorch.log_model(model, artifact_path=\"model\")\n",
    "\n",
    "    # --- 5. Testing ---\n",
    "    # Once the fine-tuning step is completed, we will test the accuracy\n",
    "    # of the model by running trainer.test():\n",
    "    print(\"\\nStarting testing...\")\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1 = evaluate_epoch(model, test_dataloader, device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, \"\n",
    "            f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "# --- 6. Prediction Example ---\n",
    "print(\"\\nExample Prediction on new data:\")\n",
    "model.eval()\n",
    "new_reviews = [\n",
    "    \"This movie was absolutely fantastic! I loved every moment of it.\",\n",
    "    \"Utterly disappointing. A complete waste of time.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"The acting was superb, but the plot was a bit weak.\"\n",
    "]\n",
    "\n",
    "for review in new_reviews:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_token_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_label_id = torch.argmax(logits, dim=-1).item()\n",
    "        predicted_sentiment = \"positive\" if predicted_label_id == 1 else \"negative\"\n",
    "        print(f\"Review: '{review}' -> Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put all the code into a file named `first_dl_with_mlflow.py` and run the command:\n",
    "```bash\n",
    "python first_dl_with_mlflow.py\n",
    "```\n",
    "\n",
    "6. Now, we can open the MLflow UI locally to see what has been logged in the local tracking server by navigating to `http://127.0.0.1:5000/`. Here, you will see that a new experiment (`dl_model_chapter02`) with a new run (`Run Name = chapter02`) has been logged:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring MLflow's components and usage patterns**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring experiments and runs in MLflow**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment** is a **first-class entity in the MLflow APIs**. **This makes sense as data scientists and ML engineers need to run lots of experiments** in order to build a working model that meets the requirements. However, **the idea of an experiment goes beyond just the model development stage and extends to the entire life cycle of the ML/DL development and deployment**. So, this means that when we do retraining or training for a production version of the model, we need to treat them as production-quality experiments. This unified view of experiments builds a bridge between the offline and online production environments. Each experiment consists of many runs where you can either **change the model parameters**, **input data**, or even **model type** for each run. So, an experiment is an umbrella entity containing a series of runs.\n",
    "\n",
    "Now, let's explore the MLflow experiments in a hands-on way. Run the following MLflow command line to interact with the tracking server:\n",
    "```bash\n",
    "mlflow experiments search\n",
    "```\n",
    "If your `MLFLOW_TRACKING_URI` environment variable points to a remote tracking server, then it will list all the experiments that you have read access to. The command lists the three columns of the experiment property: Experiment Id (an integer), Name (a text field that can be used to describe the experiment name), and Artifact Location (by default, this is located in the mlruns folder underneath the directory where you execute the MLflow commands). The mlruns folder is used by a filesystem-based MLflow tracking server to store all the metadata of experiment runs and artifacts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **THE COMMAND-LINE INTERFACE (CLI) VERSUS REST APIS VERSUS PROGRAMMING LANGUAGE-SPECIFIC APIS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow provides three different types of tools and APIs to interact with the tracking server. Here, the CLI is used so that we can explore the MLflow components."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's explore a specific MLflow experiment, as follows:\n",
    "\n",
    "1. First, create a new experiment using the MLflow CLI, as follows:\n",
    "```bash\n",
    "mlflow experiments create -n dl_model_chapter02\n",
    "```\n",
    "The preceding command creates a new experiment named `dl_model_chapter02`. If you have already run the first DL model with MLflow autologging in the previous section, the preceding command will cause an error message\n",
    "2. Now, let's examine the relationship between experiments and runs. If you look carefully at the URL of the run page you will see something similar to the following:\n",
    "`http://127.0.0.1:5000/#/experiments/1/runs/2f2ec6e72a5042f891abe0d3a533eec7` As you might have gathered, the integer after the experiments path is the experiment ID. Then, after the experiment ID, there is a runs path, followed by a GUID-like random string, which is the run ID. So, now we understand how the runs are organized under the experiment with a globally unique ID (called a run ID).\n",
    "\n",
    "Knowing a run's globally unique ID is very useful. This is because we can retrieve the run's logged data using `run_id`. If you use the mlflow runs describe `--run-id <run_id>` command line, you can get the list of metadata that this run has logged. For the experiment we just ran, the following shows the full command with the run ID:\n",
    "```bash\n",
    "mlflow runs describe –-run-id 9b1034c2b4c54125ae78705b62514b1c\n",
    "```\n",
    "\n",
    "This will present all the metadata about this run in JSON format. This metadata includes parameters used by the model training; metrics for measuring the accuracy of the model in training, validation, and testing; and more. The same data is also presented in the MLflow UI. Note that the powerful MLflow CLI will allow very convenient exploration of the MLflow logged metadata and artifacts as well as enabling shell script-based automation, as we will explore in the upcoming chapters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring MLflow models and their usages**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **THE MLFLOW TRACKING SERVER'S BACKEND STORE AND ARTIFACT STORE**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLflow tracking server has two types of storage: first, a backend store, which stores experiments and runs metadata along with params, metrics, and tags for runs; and second, an artifact store, which stores larger files such as serialized models, text files, or even generated plots for visualizing model results. For the purpose of simplicity, in this chapter, we are using a local filesystem for both the backend store and the artifact store. However, some of the more advanced features such as model registry are not available in a filesystem-based artifact store. In later chapters, we will learn how to use a model registry.\n",
    "\n",
    "Let's look at the list of artifacts, one by one:\n",
    "* `model_summary.txt`: At the root folder level, this file looks similar to the following output if you click on it. It describes the model metrics and the layers of the DL model and provides a quick overview of what the DL model looks like in terms of the number and type of neural network layers, the number and size of the parameters, and the type of metrics used in training and validation. This is very helpful when the DL model architecture is needed to be shared and communicated among team members or stakeholders.\n",
    "* The `model` folder: This folder contains a subfolder, called data, and three files called `MLmodel`, `conda.yaml`, and `requirements.txt`:\n",
    "  * `MLmodel`: This file describes the flavor of the model that MLflow supports. Flavor is MLflow-specific terminology. It describes how the model is saved, serialized, and loaded. For our first DL model, the following information is stored in an `MLmodel` file\n",
    "  * `conda.yaml`: This is a conda environment definition file used by the model to describe our dependencies\n",
    "  * `requirements.txt`: This is a Python pip-specific dependency definition file. It is just like the pip section in the conda.yaml file, as shown in Figure 2.16.\n",
    "data: This is a folder that contains the actual serialized model, called model.pth, and a description file, called pickle_module_info.txt, whose content for our first DL experiment is as follows: `mlflow.pytorch.pickle_module`\n",
    "\n",
    "This means the model is serialized using a PyTorch-compatible pickle serialization method provided by MLflow. This allows MLflow to load the model back to memory later if needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODEL REGISTRY VERSUS MODEL LOGGING**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLflow model registry requires a relational database such as MySQL as the artifact store, not just a plain filesystem. Therefore, in this chapter, we will not explore it yet. Note that a model registry is different from model logging in that, for each run, you want to log model metadata and artifacts. However, only for certain runs that meet your production requirements, you may want to register them in the model registry for production deployment and version control. In later chapters, we will learn how to register models.\n",
    "\n",
    "By now, you should have a good understanding of the list of files and metadata about the model and the serialized model (along with the .pth file extension in our experiment, which refers to a PyTorch serialized model) logged in the MLflow artifact store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring MLflow code tracking and its usages**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring the metadata of the run, we can also discover how the code is being tracked. As shown in the MLflow UI and the command-line output in JSON, the code is tracked in three ways: a filename, a Git commit hash, and a source type. You can execute the following command line:\n",
    "\n",
    "```bash\n",
    "mlflow runs describe --run-id 9b1034c2b4c54125ae78705b62514b1c | grep mlflow.source\n",
    "```\n",
    "Based on this 9b1034c2b4c54125ae78705b62514b1c Git commit hash, we can go on to find the exact copy of the Python code we used.\n",
    "\n",
    "Note that, here, the source type is `LOCAL`. This means that we execute the MLflow-enabled source code from a local copy of the code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LOCAL VERSUS REMOTE GITHUB CODE**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the source is a local copy of the code, there is a caveat regarding the Git commit hash that you see in the MLflow tracking server. If you make code changes locally but forget to commit them and then immediately start an MLflow experiment tracking run, MLflow will only log the most recent Git commit hash. We can solve this problem in one of two ways:\n",
    "\n",
    "1. Commit our code changes before running the MLflow experiment\n",
    "2. Use remote GitHub code to run the experiment.\n",
    "\n",
    "Since the first method is not easily enforceable, the second method is preferred. Using remote GitHub code to run a DL experiment is an advanced topic that we will explore in later chapters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have learned about the MLflow tracking server, experiments, and runs. Additionally, we have logged metadata about runs such as parameters and metrics, examined code tracking, and explored model logging. These tracking and logging capabilities ensure that we have a solid ML experiment management system, not only for model development but also for model deployment in the future, as we need to track which runs produce the model for production. Reproducibility and provenance-tracking are the hallmarks of what MLflow provides. In addition to this, MLflow provides other components such as MLproject for standardized ML project code organization, a model registry for model versioning control, model deployment capabilities, and model explainability tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
