{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 : Deep learning Life Cycle and MLOps Challenges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While research in areas of deep learning has made giant leaps, bringing these DL models from offline experimentation to production and continuously improving the models to deliver sustainable values is still a challenge. For example, a recent article by VentureBeat (https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/) found that $87\\%$ of data science projects never make it to production. While there might be business reasons for such a low production rate, a major contributing factor is the difficulty caused by the lack of experiment management and a mature model production and feedback platform.\n",
    "\n",
    "This chapter will help us to understand the challenges and bridge these gaps by learning the concepts, steps, and components that are commonly used in the full life cycle of DL model development. Additionally, we will learn about the challenges of an emerging field known as Machine Learning Operations (MLOps), which aims to standardize and automate ML life cycle development, deployment, and operation. Having a solid understanding of these challenges will motivate us to learn the skills presented in the rest of this book using MLflow, an open source, ML full life cycle platform. The business values of adopting MLOps' best practices are numerous; they include faster time-to-market of model-derived product features, lower operating costs, agile A/B testing, and strategic decision making to ultimately improve customer experience"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understand the DL file cycle and MLOps challenges**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the DL life cycle and MLOps challenges\n",
    "Nowadays, the most successful DL models that are deployed in production primarily observe the following two steps:\n",
    "\n",
    "1. **Self-supervised learning**: This refers to the pretraining of a model in a data-rich domain that does not require labeled data. This step produces a pretrained model, which is also called a foundation model, for example, BERT, GPT-3 for NLP, and VGG-NETS for computer vision.\n",
    "2. **Transfer learning**: This refers to the fine-tuning of the pretrained model in a specific prediction task such as text sentiment classification, which requires labeled training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DL over classical ML**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike classical ML model development, where, usually, a feature engineering step is required to extract and transform raw data into features to train an ML model such as decision tree or logistic regression, DL can learn the features automatically, which is especially attractive for modeling unstructured data such as texts, images, videos, audio, and speeches. DL is also called representational learning due to this characteristic. In addition to this, DL is usually data- and compute-intensive, requiring Graphics Process Units (GPUs), Tensor Process Units (TPU), or other types of computing hardware accelerators for at-scale training and inference. Explainability for DL models is also harder to implement, compared with traditional ML models, although recent progress has now made that possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementing a basic DL sentiment classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m80 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m74 packages\u001b[0m \u001b[2min 0.08ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m83 packages\u001b[0m \u001b[2min 774ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m     0 B/3.29 MiB            \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 16.00 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 32.00 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 32.00 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 40.32 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 43.09 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 59.09 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 62.52 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 78.52 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 79.16 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 79.16 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 95.16 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 96.00 KiB/3.29 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 112.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 128.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 144.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 160.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 176.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 192.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 208.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 224.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 240.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 256.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 272.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 288.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 304.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 320.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 320.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 336.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 352.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 368.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 432.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 512.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 592.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 672.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 784.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 880.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 976.00 KiB/3.29 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.02 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.12 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.23 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.36 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.44 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.58 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 1.67 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)m-------------\u001b[0m\u001b[0m 1.80 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)2m------------\u001b[0m\u001b[0m 1.91 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)[2m-----------\u001b[0m\u001b[0m 2.05 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\u001b[2m----------\u001b[0m\u001b[0m 2.17 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-\u001b[2m---------\u001b[0m\u001b[0m 2.30 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--\u001b[2m--------\u001b[0m\u001b[0m 2.41 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---\u001b[2m-------\u001b[0m\u001b[0m 2.52 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-----\u001b[2m-----\u001b[0m\u001b[0m 2.69 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)------\u001b[2m----\u001b[0m\u001b[0m 2.80 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------\u001b[2m--\u001b[0m\u001b[0m 2.97 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---------\u001b[2m-\u001b[0m\u001b[0m 3.08 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----------\u001b[2m\u001b[0m\u001b[0m 3.22 MiB/3.29 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 2.16s\u001b[0m\u001b[0m                                                  \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 46ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add torchmetrics torchinfo\n",
    "!uv add torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. Data Download Utility (similar to Flash's download_data) ---\n",
    "def download_and_extract_zip(url, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    zip_file_name = os.path.join(path, \"downloaded_data.zip\")\n",
    "\n",
    "    print(f\"Downloading data from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 # 1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(zip_file_name, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong during download\")\n",
    "        return\n",
    "\n",
    "    print(f\"Extracting {zip_file_name} to {path}...\")\n",
    "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path)\n",
    "    os.remove(zip_file_name) # Clean up the zip file\n",
    "    print(\"Download and extraction complete.\")\n",
    "\n",
    "# --- 2. Custom Dataset for IMDB (replaces TextClassificationData) ---\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Ensure 'sentiment' is mapped to numerical labels\n",
    "        # 0 for negative, 1 for positive\n",
    "        self.label_map = {'negative': 0, 'positive': 1}\n",
    "        self.dataframe['sentiment_label'] = self.dataframe['sentiment'].map(self.label_map)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.dataframe.loc[idx, 'review'])\n",
    "        label = self.dataframe.loc[idx, 'sentiment_label']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt', # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Training Function ---\n",
    "# \"\"\"\n",
    "# Once we have the data, we can now perform fine-tuning using a foundation model.\n",
    "# First, we declare classifier_model by calling TextClassifier with a backbone\n",
    "# assigned to prajjwal1/bert-tiny (which is a much smaller BERT-like pretrained\n",
    "# model located in the Hugging Face model repository: https://huggingface.co/prajjwal1/bert-tiny).\n",
    "# This means our model will be based on the bert-tiny model.\n",
    "# \"\"\"\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# --- 4. Evaluation Function ---\n",
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    # Get precision, recall, f1-score for positive class (label 1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true_labels, all_predictions, average='binary', pos_label=1\n",
    "    )\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://pl-flash-data.s3.amazonaws.com/imdb.zip...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a4611e4cae4da0a85b294a14fc0602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/15.9M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/downloaded_data.zip to ./data/...\n",
      "Download and extraction complete.\n",
      "Number of classes: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d533d3929e0d492386d05e019199ebf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4503e8ba53fd452da8dee4bfece56897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d98ff6c0ac4e8f8964ee6d489fea42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98b2d19f5b641598890b130e4b901d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- Main Script ---\n",
    "# if __name__ == \"__main__\":\n",
    "# --- 1. & 2. Data Download and Preparation ---\n",
    "data_url = \"https://pl-flash-data.s3.amazonaws.com/imdb.zip\"\n",
    "data_path = \"./data/\"\n",
    "download_and_extract_zip(data_url, data_path)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_path, \"imdb/train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(data_path, \"imdb/valid.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_path, \"imdb/test.csv\"))\n",
    "\n",
    "# Determine number of classes from the sentiment column\n",
    "num_classes = train_df['sentiment'].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Initialize Tokenizer\n",
    "model_name = \"prajjwal1/bert-tiny\" # As used in Flash example\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_token_length = 128 # A common max length for BERT-tiny, adjust as needed\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = IMDBDataset(train_df, tokenizer, max_token_length)\n",
    "val_dataset = IMDBDataset(val_df, tokenizer, max_token_length)\n",
    "test_dataset = IMDBDataset(test_df, tokenizer, max_token_length)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# --- 3. Model Definition and Setup ---\n",
    "# Use AutoModelForSequenceClassification for classification tasks with Hugging Face models\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Learning Rate\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # Common learning rate for fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b455cb003b7943d892f13fa33b6952dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d657c8eecbe547e9a839ab3cacdfde11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6459\n",
      "Validation Loss: 0.5599, Accuracy: 0.7236, Precision: 0.7689, Recall: 0.6225, F1: 0.6880\n",
      "--- Epoch 2/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5307d140d93471eafc240c84c771796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec84b5e91c74e64a218885d9d80af15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5087\n",
      "Validation Loss: 0.4703, Accuracy: 0.7748, Precision: 0.7738, Recall: 0.7631, F1: 0.7684\n",
      "--- Epoch 3/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a0560a67f342faa4c23da1cea5f68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a018f8ff98244725a6c4cfa7374adec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4406\n",
      "Validation Loss: 0.4451, Accuracy: 0.7872, Precision: 0.7932, Recall: 0.7647, F1: 0.7787\n",
      "--- Epoch 4/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa66ba86f30c4aa1b1a84f03026ec324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336fe8f441cf49d69bb925b7806f96bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4091\n",
      "Validation Loss: 0.4278, Accuracy: 0.7980, Precision: 0.8201, Recall: 0.7525, F1: 0.7848\n",
      "--- Epoch 5/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61ef620a0944eff8dcb39c74e29592b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fa21f1b65640919ab6a3778f4bc8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3804\n",
      "Validation Loss: 0.4183, Accuracy: 0.8048, Precision: 0.8002, Recall: 0.8015, F1: 0.8008\n",
      "--- Epoch 6/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc5904453f04f56abd989eaaa4ff301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96ef03e9ffb434e9e320e991a36597e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3591\n",
      "Validation Loss: 0.4171, Accuracy: 0.8056, Precision: 0.7838, Recall: 0.8325, F1: 0.8074\n",
      "--- Epoch 7/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bad8b4e64f4c46a51610edb2b7a37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f8ee183cbe40ac970bb99b58dad051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3371\n",
      "Validation Loss: 0.4123, Accuracy: 0.8196, Precision: 0.8229, Recall: 0.8047, F1: 0.8137\n",
      "--- Epoch 8/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1079ca56b74bfb8db65de99f398f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7028f1031ba4b35aa4a6d65d788da47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3143\n",
      "Validation Loss: 0.4205, Accuracy: 0.8100, Precision: 0.8484, Recall: 0.7451, F1: 0.7934\n",
      "--- Epoch 9/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be9b1a9518543bd8d3431cb6cb2c653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74237a2331b4329a7e1d86f6cbf7131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2931\n",
      "Validation Loss: 0.4285, Accuracy: 0.8144, Precision: 0.8578, Recall: 0.7443, F1: 0.7970\n",
      "--- Epoch 10/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73f6e8f88444e609dd92e1755c73ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e485f40558bb440fa88e4d512853c5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2736\n",
      "Validation Loss: 0.4441, Accuracy: 0.8132, Precision: 0.8636, Recall: 0.7345, F1: 0.7938\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Training Loop ---\n",
    "num_epochs = 10 # As specified in PyTorch Lightning example\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate_epoch(model, val_dataloader, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, \"\n",
    "            f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DL FINE-TUNING TIME**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your running environment, the fine-tuning step might take a couple of minutes on a GPU or around 10 minutes (if you're only using a CPU). You can reduce max_epochs=1 if you simply want to get a basic version of the sentiment classifier quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5f450f21d543e1916175a4a5a70a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4693, Accuracy: 0.8036, Precision: 0.8732, Recall: 0.7147, F1: 0.7861\n",
      "\n",
      "Example Prediction on new data:\n",
      "Review: 'This movie was absolutely fantastic! I loved every moment of it.' -> Predicted Sentiment: positive\n",
      "Review: 'Utterly disappointing. A complete waste of time.' -> Predicted Sentiment: negative\n",
      "Review: 'It was okay, nothing special.' -> Predicted Sentiment: negative\n",
      "Review: 'The acting was superb, but the plot was a bit weak.' -> Predicted Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Testing ---\n",
    "# Once the fine-tuning step is completed, we will test the accuracy\n",
    "# of the model by running trainer.test():\n",
    "print(\"\\nStarting testing...\")\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1 = evaluate_epoch(model, test_dataloader, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, \"\n",
    "        f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "# --- 6. Prediction Example ---\n",
    "print(\"\\nExample Prediction on new data:\")\n",
    "model.eval()\n",
    "new_reviews = [\n",
    "    \"This movie was absolutely fantastic! I loved every moment of it.\",\n",
    "    \"Utterly disappointing. A complete waste of time.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"The acting was superb, but the plot was a bit weak.\"\n",
    "]\n",
    "\n",
    "for review in new_reviews:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_token_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_label_id = torch.argmax(logits, dim=-1).item()\n",
    "        predicted_sentiment = \"positive\" if predicted_label_id == 1 else \"negative\"\n",
    "        print(f\"Review: '{review}' -> Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding DL's full life cycle development**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have gathered that the core DL development paradigm revolves around three key artifacts: Data, Model, and Code. In addition to this, Explainability is another major artifact that is required in many mission-critical application scenarios such as medical diagnoses, the financial industry, and decision making for criminal justice.\n",
    "\n",
    "Once we have a model that's good enough for the use cases and customer scenarios, the complexity increases as we need a way to continuously deploy and update the model in production, monitor the model and data drift, and then retrain the model when necessary. This complexity further increases when at-scale training, deployment, monitoring, and explainability are needed.\n",
    "\n",
    "Let's examine what a DL life cycle looks like (see Figure 1.3). There are five stages:\n",
    "\n",
    "1. Data collection, cleaning, and annotation/labeling.\n",
    "2. Model development (which is also known as offline experimentation). The core DL development paradigm in Figure 1.1 is considered part of the model development stage, which itself can be an iterative process.\n",
    "3. Model deployment and serving in production.\n",
    "4. Model validation and A/B testing (which is also known as online experimentation; this is usually in a production environment).\n",
    "5. Monitoring and feedback data collection during production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding MLOps challenges**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLOps has some connections to DevOps, where a set of technology stacks and standard operational procedures are used for software development and deployment combined with IT operations. Unlike traditional software development, ML and especially DL represent a new era of software development paradigms called Software 2.0.\n",
    "\n",
    "* Here are the three foundation layers:\n",
    "  * Infrastructure management and automation\n",
    "  * Application life cycle management and Continuous Integration and Continuous Deployment (CI/CD)\n",
    "  * Service system observability\n",
    "* Here are the four pillars:\n",
    "  * Data observability and management\n",
    "  * Model observability and life cycle management\n",
    "  * Explainability and Artificial Intelligence (AI) observability\n",
    "  * Code reproducibility and observability\n",
    "\n",
    "Additionally, we will explain MLflow's roles in these MLOps layers and pillars so that we have a clear picture regarding what MLflow can do to build up the MLOps layers in their entirety:\n",
    "\n",
    "* **Infrastructure management and automation**: This includes, but is not limited to, Kubernetes (also known as k8s) for automated container orchestration and Terraform (commonly used for managing hundreds of cloud services and access control). These tools are adapted to manage ML and DL applications that have deployed models as service endpoints. These infrastructure layers are not the focus of this book; instead, we will focus on how to deploy a trained DL model using MLflow's provided capabilities.\n",
    "\n",
    "* **Application life cycle management and CI/CD**: This includes, but is not limited to, Docker containers for virtualization, container life cycle management tools such as Kubernetes, and CircleCI or Concourse for CI and CD. Usually, CI means that whenever there are code or model changes in a GitHub repository, a series of automatic tests will be triggered to make sure no breaking changes are introduced. Once these tests have been passed, new changes will be automatically released as part of a new package. This will then trigger a new deployment process (CD) to deploy the new package to the production environment (often, this will include human approval as a safety gate). Note that these tools are not unique to ML applications but have been adapted to ML and DL applications, especially when we require GPU and distributed clusters for the training and testing of DL models. In this book, we will not focus on these tools but will mention the integration points or examples when needed.\n",
    "\n",
    "* **Service system observability**: This is mostly for monitoring the hardware/clusters/CPU/memory/storage, operating system, service availability, latency, and throughput. This includes tools such as Grafana, Datadog, and more. Again, these are not unique to ML and DL applications and are not the focus of this book.\n",
    "\n",
    "* **Data observability and management**: This is traditionally under-represented in the DevOps world but becomes very important in MLOps as data is critical within the full life cycle of ML/DL models. This includes data quality monitoring, outlier detection, data drift and concept drift detection, bias detection, secured and compliant data sharing, data provenance tracking and versioning, and more. The tool stacks in this area that are suitable for ML and DL applications are still emerging. A few examples include DataFold (https://www.datafold.com/) and Databand (https://databand.ai/open-source/). A recent development in data management is a unified lakehouse architecture and implementation called Delta Lake (http://delta.io) that can be used for ML data management. MLflow has native integration points with Delta Lake, and we will cover that integration in this book.\n",
    "\n",
    "* **Model observability and life cycle management**: This is unique to ML/DL models, and it only became widely available recently due to the rise of MLflow. This includes tools for model training, testing, versioning, registration, deployment, serialization, model drift monitoring, and more. We will learn about the exciting capabilities that MLflow provides in this area. Note that once we combine CI/CD tools with MLflow training/monitoring, user feedback loops, and human annotations, we can achieve Continuous Training, Continuous Testing, and Continuous Labeling. MLflow provides the foundational capabilities so that further automation in MLOps becomes possible, although such complete automation will not be the focus of this book. Interested readers can find relevant references at the end of this chapter to explore this area further.\n",
    "\n",
    "* **Explainability and AI observability**: This is unique to ML/DL models and is especially important for DL models, as traditionally, DL models are treated as black boxes. Understanding why the model provides certain predictions is critical for societally important applications. For example, in medical, financial, juridical, and many human-in-the-loop decision support applications, such as civilian and military emergency response, the demand for explainability is increasingly higher. MLflow provides native integration with a popular explainability framework called SHAP, which we will cover in this book.\n",
    "\n",
    "* **Code reproducibility and observability**: This is not entirely unique to ML/DL applications. However, DL models face some special challenges as the number of DL code frameworks are diverse and the need to reproduce a model is not entirely up to the code alone (we also need data and execution environments such as GPU clusters). In addition to this, notebooks are commonly used in model development and production. How to manage the notebooks along with the model run is important. Usually, GitHub is used to manage the code repository; however, we need to structure the ML project code in a way that's reproducible either locally (such as on a local laptop) or remotely (for example, in a Databricks' GPU cluster). MLflow provides this capability to allow DL projects that have been written once to run anywhere, whether this is in an offline experimentation environment or an online production environment. We will cover MLflow's MLproject capability in this book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
